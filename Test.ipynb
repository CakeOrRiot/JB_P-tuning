{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[Статья](https://aclanthology.org/2020.lrec-1.299.pdf)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d04cbce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2Tokenizer,GPT2Model\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a319a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class SonnetData(Dataset):\n",
    "    def __init__(self, number_of_sonnets, data_path):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "        self.sonnets = []\n",
    "        for i in range(number_of_sonnets):\n",
    "            with open(data_path / f'{i + 1}', 'rt') as f:\n",
    "                self.sonnets.append(\n",
    "                    torch.tensor(self.tokenizer(f.readlines(),is_split_into_words=True)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sonnets)\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        return self.sonnets[ind]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "You need to instantiate GPT2TokenizerFast with add_prefix_space=True to use it with pretokenized inputs.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_522865/1104261804.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mnumber_of_sonnets\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m153\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mdata_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPath\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'./data/processed/'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSonnetData\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnumber_of_sonnets\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdata_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_522865/2497201759.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, number_of_sonnets, data_path)\u001B[0m\n\u001B[1;32m      6\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_path\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0;34mf'{i + 1}'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'rt'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m                 self.sonnets.append(\n\u001B[0;32m----> 8\u001B[0;31m                     torch.tensor(self.tokenizer(f.readlines(),is_split_into_words=True)))\n\u001B[0m\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__len__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2461\u001B[0m             )\n\u001B[1;32m   2462\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2463\u001B[0;31m             return self.encode_plus(\n\u001B[0m\u001B[1;32m   2464\u001B[0m                 \u001B[0mtext\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2465\u001B[0m                 \u001B[0mtext_pair\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtext_pair\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001B[0m in \u001B[0;36mencode_plus\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2534\u001B[0m         )\n\u001B[1;32m   2535\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2536\u001B[0;31m         return self._encode_plus(\n\u001B[0m\u001B[1;32m   2537\u001B[0m             \u001B[0mtext\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2538\u001B[0m             \u001B[0mtext_pair\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtext_pair\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py\u001B[0m in \u001B[0;36m_encode_plus\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    167\u001B[0m         \u001B[0mis_split_into_words\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"is_split_into_words\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 169\u001B[0;31m         assert self.add_prefix_space or not is_split_into_words, (\n\u001B[0m\u001B[1;32m    170\u001B[0m             \u001B[0;34mf\"You need to instantiate {self.__class__.__name__} with add_prefix_space=True \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    171\u001B[0m             \u001B[0;34m\"to use it with pretokenized inputs.\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAssertionError\u001B[0m: You need to instantiate GPT2TokenizerFast with add_prefix_space=True to use it with pretokenized inputs."
     ]
    }
   ],
   "source": [
    "number_of_sonnets = 153\n",
    "data_path = Path('./data/processed/')\n",
    "data = SonnetData(number_of_sonnets,data_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}