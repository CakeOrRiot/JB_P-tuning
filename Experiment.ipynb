{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr8ueJpxWzW1",
        "outputId": "75d4c13b-e009-47f0-9b96-341f5bb342bf"
      },
      "id": "mr8ueJpxWzW1",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "1wHUDFadWrEu"
      },
      "id": "1wHUDFadWrEu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d04cbce5",
      "metadata": {
        "id": "d04cbce5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2TokenizerFast\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import pipeline\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": [
        "class SonnetData:\n",
        "    def __init__(self, data_path):\n",
        "        self.sonnets = []\n",
        "        self.path = data_path\n",
        "        for file_name in os.listdir(data_path):\n",
        "            with open(data_path / file_name, 'rt') as f:\n",
        "              text = ''.join(f.readlines())\n",
        "              self.sonnets.append(text)\n",
        "        \n",
        "    def tokenize(self, tokenizer):\n",
        "      self.sonnets = tokenizer(self.sonnets, truncation=True,padding=True)\n",
        "\n",
        "    def group_texts(self,block_size=16):\n",
        "      concatenated_examples = {k: sum(self.sonnets[k], []) for k in self.sonnets.keys()}\n",
        "      total_length = len(concatenated_examples[list(self.sonnets.keys())[0]])\n",
        "      self.sonnets = {\n",
        "          k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "          for k, t in concatenated_examples.items()\n",
        "      }\n",
        "      self.sonnets['labels'] = self.sonnets['input_ids'].copy()\n",
        "      if len(self.sonnets['input_ids'][-1])!=len(self.sonnets['input_ids'][0]):\n",
        "        for x in self.sonnets:\n",
        "          self.sonnets[x].pop()\n",
        "    def __len__(self):\n",
        "        return len(self.sonnets['input_ids'])\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        return self.sonnets['input_ids'][ind]"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "puFQK6p9V80Q"
      },
      "id": "puFQK6p9V80Q"
    },
    {
      "cell_type": "code",
      "source": [
        "def process(data,tokenizer):\n",
        "  data.tokenize(tokenizer)\n",
        "  data.group_texts()\n",
        "  for key in data.sonnets:\n",
        "    # print(key,data.sonnets[key])\n",
        "    data.sonnets[key] = torch.tensor(data.sonnets[key])"
      ],
      "metadata": {
        "id": "6-IYu0JfhNzz"
      },
      "id": "6-IYu0JfhNzz",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "source": [
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "\n",
        "number_of_sonnets = 153\n",
        "data_path = Path('/content/drive/MyDrive/JB/P-tuning/data/')\n",
        "train = SonnetData(data_path / 'train')\n",
        "test = SonnetData(data_path / 'test')\n",
        "validate = SonnetData(data_path / 'validate')\n",
        "\n",
        "process(train,tokenizer)\n",
        "process(test,tokenizer)\n",
        "process(validate,tokenizer)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ll_lLud-V80Q"
      },
      "id": "ll_lLud-V80Q"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 1441\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 543\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='543' max='543' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [543/543 00:38, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>4.989789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>4.892489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.972800</td>\n",
              "      <td>4.882153</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 169\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 169\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./results/checkpoint-500\n",
            "Configuration saved in ./results/checkpoint-500/config.json\n",
            "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 169\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=543, training_loss=4.945821547815593, metrics={'train_runtime': 38.596, 'train_samples_per_second': 112.007, 'train_steps_per_second': 14.069, 'total_flos': 17649778950144.0, 'train_loss': 4.945821547815593, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    \n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=test,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "id": "urwnJb4sV80S",
        "outputId": "f0220d8d-6267-4a6b-df06-efbb82a656fc"
      },
      "id": "urwnJb4sV80S"
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(\"cpu\")\n",
        "tuned_model_pipeline = pipeline(\"text-generation\",model=model,tokenizer=tokenizer)\n",
        "tuned_model_pipeline(\"How oft, when thou, my music, music play'st Upon that blessed\")[0]['generated_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ztgDk6bEAjN7",
        "outputId": "95d4aac3-e855-46c0-bbea-8371c3b939df"
      },
      "id": "ztgDk6bEAjN7",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"How oft, when thou, my music, music play'st Upon that blessed truth,\\nThe beauty of thy sweet love:\\nMy most love, in my love's life, shall be given\\nOf the sweet love. So thou art when\""
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
        "original_tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "original_model_pipeline = pipeline(\"text-generation\",model=original_model,tokenizer=original_tokenizer)\n"
      ],
      "metadata": {
        "id": "Y-OR1eIpYglZ"
      },
      "id": "Y-OR1eIpYglZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_model_pipeline(\"How oft, when thou, my music, music play'st Upon that blessed\")[0]['generated_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "_5JRtsUepDEU",
        "outputId": "abf6401e-9cf0-4d39-a381-b6bd7499b93f"
      },
      "id": "_5JRtsUepDEU",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'How oft, when thou, my music, music play\\'st Upon that blessed me, how do we know to that song?\"\\n\\n\\n\\n\\n\\nâ€”\\n\\n\\nWe could have made something of it and now it is. There are many'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5lzZJAFTrNCF"
      },
      "id": "5lzZJAFTrNCF",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "Experiment.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}